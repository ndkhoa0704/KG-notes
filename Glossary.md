# A
# B
# C
- **Concentric circles:** two or more circles which have the same center. The region between the circles is called an annulus. [More information](https://machinelearningcoban.com/2017/05/31/matrixfactorization/)
- **Connectivity patterns:** connectivity, relational pattern between entities in a knowledge bases. [Common connectivity patterns](https://aws-dglke.readthedocs.io/en/latest/kg.html#common-connectivity-patterns)
- **Collaborative filtering:**
- **Collective matrix factorization:**
# D
# E
- **Energy-based models:**  probabilistic model governed by an energy function that describes the probability of a certain state
- **Energy function:** a function that should be minimized
- **Embedding:** a dense presentation of a knowledge graph in a continuous, low-dimensional vector space
- **Expressiveness:** the measures of complexity of functions that can possibly be computed by a parametric function such as a neural net. [More information](https://blog.evjang.com/2017/11/exp-train-gen.html)
# F
# G 
- **Geometric models:** models that interpret relations as geometric transformation
# H
# I
# J
# K
# L
# M
- **Matrix factorization**: a technique to decompose a large matrix in to multiple smaller matrix in order to reduce the storage space
- **Multi-relational data**: directed graphs whose nodes are entities, edges are relations between nodes 
# N
- **Negative sampling**: corrupting a triplet $\langle h, r, t \rangle$. Either $h$ or $t$ or corrupted by sampling heads and tails from KG
# O 
# P
- **Pairwise ranking loss:** 
    - A pair of similar sample is called positive pair
	- A pair of dissimilar sample is called negative pair
	- Aim to increase/decrease distance between neg/pos pairs
	- [More information](https://gombru.github.io/2019/04/03/ranking_loss/)
# Q 
# R
- **Ranking loss:** computes the criterion to predict the distances between inputs. [More information](https://analyticsindiamag.com/all-pytorch-loss-function/#h-9-margin-ranking-loss-nn-marginrankingloss)
# S
# T
- **Tensor factorization:**
- **Triplet ranking loss (see Pairwise ranking loss):** Composed with both negative pair and positive pair. The objective is that the distance between the anchor sample and the negative sample representations  is greater (and bigger than a margin m) than the distance between the anchor and positive representations. [More information](https://gombru.github.io/2019/04/03/ranking_loss/)
# U
# V
# W
# X
# Y
# Z