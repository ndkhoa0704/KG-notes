# A
# B
# C
- **Concentric circles:** two or more circles which have the same center. The region between the circles is called an annulus. [More information](https://machinelearningcoban.com/2017/05/31/matrixfactorization/)
- **Connectivity patterns:** connectivity, relational pattern between entities in a knowledge bases. [Common connectivity patterns](https://aws-dglke.readthedocs.io/en/latest/kg.html#common-connectivity-patterns)
- **Collaborative filtering:**
- **Collective matrix factorization:**
- **Collective rankding loss:** 
# D
# E
- **Energy-based models:**  probabilistic model governed by an energy function that describes the probability of a certain state
- **Energy function:** a function that should be minimized
- **Embedding:** a dense presentation of a knowledge graph in a continuous, low-dimensional vector space
- **Expressiveness:** the measures of complexity of functions that can possibly be computed by a parametric function such as a neural net. [More information](https://blog.evjang.com/2017/11/exp-train-gen.html)
# F
- **Fuzzy logic:** an approach to computing based on "degrees of truth" rather than the usual "true or false" (1 or 0) Boolean logic on which the modern computer is based. [More information](https://www.techtarget.com/searchenterpriseai/definition/fuzzy-logic#:~:text=Fuzzy%20logic%20is%20an%20approach,at%20Berkeley%20in%20the%201960s.)
# G 
- **Geometric models:** models that interpret relations as geometric transformation
- **Gated recurrent unit**: gating mechanism of RNN. [More information](https://en.wikipedia.org/wiki/Gated_recurrent_unit) 
- **Golden triplets (KGE):** a tripipiplipletetsdfasetedfhrew
# H
- **Hypernetwoeks:** a network that generate weight for another network. [More information](https://paperswithcode.com/method/hypernetwork)
# I
# J
- **Joint embedding:** 
# K
# L
# M
- **Matrix factorization**: a technique to decompose a large matrix in to multiple smaller matrix in order to reduce the storage space
- **Multi-relational data**: directed graphs whose nodes are entities, edges are relations between nodes 
- **Multirelational graph:** each edges are labelled, have direction so that they can specify the types of relationship between entities. [More information](https://www.slideshare.net/slidarko/multirelational-graph-structures-from-algebra-to-application-3879972)
# N
- **Negative sampling**: corrupting a triplet $\langle h, r, t \rangle$. Either $h$ or $t$ or corrupted by sampling heads and tails from KG
# O 
# P
- **Pairwise ranking loss:** 
    - A pair of similar sample is called positive pair
	- A pair of dissimilar sample is called negative pair
	- Aim to increase/decrease distance between neg/pos pairs
	- [More information](https://gombru.github.io/2019/04/03/ranking_loss/)
- **P-norm:** is a function from a real or complex vector space to the non-negative real numbers that behaves in certain ways like the distance from the origin. [More information](https://hackmd.io/9S-mya3FQ7mfsx4k3F6P7g?both)
# Q 
# R
- **Ranking loss:** computes the criterion to predict the distances between inputs. [More information](https://analyticsindiamag.com/all-pytorch-loss-function/#h-9-margin-ranking-loss-nn-marginrankingloss)
- **Recurrent network**: 
- **Relation structure:** given a relation
	- **1-1:** one entity links to **at most** 1 entity through this relation
	- **1-N:** one entitiy links to **more than** one entity through this relation
	- **N-1:** more than one entity connect to the same entity through this relation
	- **N-N:**  one entity **can link** to more than one entity through this relation and one entity **could be linked** by more than one entity
	- **1-N-1:** there are **more than one** entity that link **one** same entity **to another** same entity
# S
- **Single-relational graph:** all edges of the graph have the same type. [More information](https://www.slideshare.net/slidarko/multirelational-graph-structures-from-algebra-to-application-3879972)
# T
- **Tensor factorization:**
- **Triplet ranking loss (see Pairwise ranking loss):** Composed with both negative pair and positive pair. The objective is that the distance between the anchor sample and the negative sample representations  is greater (and bigger than a margin m) than the distance between the anchor and positive representations. [More information](https://gombru.github.io/2019/04/03/ranking_loss/)
# U
# V
# W
- **Word embedding:** a type of word representation that allows words with similar meaning to have a similar representation. [More information](https://machinelearningmastery.com/what-are-word-embeddings/)
# X
# Y
# Z